{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fa45011-b6f3-4641-83d5-f8b28baac7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4d10dac-665a-4e9b-977a-61da6a14685b",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-1.0-pro-latest:generateContent",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     37\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a poem about a robot who falls in love with a human.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 38\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43msend_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m generated_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerations\u001b[39m\u001b[38;5;124m\"\u001b[39m, [{}])[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 33\u001b[0m, in \u001b[0;36msend_request\u001b[1;34m(prompt, temperature)\u001b[0m\n\u001b[0;32m     26\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt,\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m128\u001b[39m,  \u001b[38;5;66;03m# Adjust as needed for longer generations\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[0;32m     30\u001b[0m }\n\u001b[0;32m     32\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(endpoint, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m---> 33\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Raise an exception for error responses\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mS:\\PythonInterpreter\\WPy64-31230\\python-3.12.3.amd64\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://generativelanguage.googleapis.com/v1beta/models/gemini-1.0-pro-latest:generateContent"
     ]
    }
   ],
   "source": [
    "# Replace with your Google Cloud project ID and API key\n",
    "project_id = \"GOOGLE__PROJECT_ID\"\n",
    "api_key = \"GOOGLE_API_KEY\"\n",
    "\n",
    "# Endpoint URL (replace with the appropriate version if needed)\n",
    "endpoint = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.0-pro-latest:generateContent\"\n",
    "\n",
    "# Function to send a request to Gemini API\n",
    "def send_request(prompt, temperature=0.7):\n",
    "  \"\"\"Sends a request to the Gemini API for text generation.\n",
    "\n",
    "  Args:\n",
    "      prompt (str): The text prompt to guide the generation.\n",
    "      temperature (float, optional): Controls the randomness of the generated text.\n",
    "          Defaults to 0.7.\n",
    "\n",
    "  Returns:\n",
    "      dict: The response from the API containing the generated text.\n",
    "  \"\"\"\n",
    "\n",
    "  headers = {\n",
    "      \"Authorization\": f\"Bearer {api_key}\",\n",
    "      \"Content-Type\": \"application/json\",\n",
    "  }\n",
    "\n",
    "  data = {\n",
    "      \"prompt\": prompt,\n",
    "      \"max_tokens\": 128,  # Adjust as needed for longer generations\n",
    "      \"temperature\": temperature,\n",
    "  }\n",
    "\n",
    "  response = requests.post(endpoint, headers=headers, json=data)\n",
    "  response.raise_for_status()  # Raise an exception for error responses\n",
    "  return response.json()\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Write a poem about a robot who falls in love with a human.\"\n",
    "response = send_request(prompt)\n",
    "\n",
    "generated_text = response.get(\"generations\", [{}])[0].get(\"text\", \"\")\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Generated Text:\\n{generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd52d29a-288a-4e66-9436-b4d521c6d0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
