{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50acff89-a25c-432e-ba08-0a1800dc0c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize, sent_tokenize, ne_chunk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6967d87d-3123-4034-b870-d50c004e9ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80e37ee3-1e8c-4764-b244-5d68e0401c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Convert treebank tags to wordnet tags.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def return_svo(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    named_entities = ne_chunk(pos_tags, binary=True)\n",
    "    \n",
    "    # Enhanced grammar for better chunking\n",
    "    grammar = r\"\"\"\n",
    "      NP: {<DT>?<JJ>*<NN.*>+}\n",
    "          {<NNP>+}\n",
    "          {<NE>+}\n",
    "      VP: {<MD>?<VB.*>+<RB>?}\n",
    "      PP: {<IN><NP>}\n",
    "      CLAUSE: {<NP><VP><NP|PP>*}\n",
    "    \"\"\"\n",
    "    \n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    tree = cp.parse(named_entities)\n",
    "    \n",
    "    svos = []\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() == 'CLAUSE':\n",
    "            subject = verb = obj = None\n",
    "            for s in subtree:\n",
    "                if s.label() == 'NP':\n",
    "                    if not subject:\n",
    "                        subject = ' '.join(token for token, pos in s.leaves())\n",
    "                    else:\n",
    "                        obj = ' '.join(token for token, pos in s.leaves())\n",
    "                elif s.label() == 'VP':\n",
    "                    verb_phrase = ' '.join(token for token, pos in s.leaves())\n",
    "                    verb = next((token for token, pos in pos_tag(word_tokenize(verb_phrase)) if pos.startswith('V')), None)\n",
    "                    if verb:\n",
    "                        verb = lemmatizer.lemmatize(verb, get_wordnet_pos(pos_tag([verb])[0][1]))\n",
    "\n",
    "            if subject and verb and obj:\n",
    "                svos.append({\n",
    "                    \"subject\": subject,\n",
    "                    \"verb\": verb,\n",
    "                    \"stem\": lemmatizer.lemmatize(verb, get_wordnet_pos(pos_tag([verb])[0][1])),\n",
    "                    \"object\": obj\n",
    "                })\n",
    "\n",
    "    return svos\n",
    "\n",
    "def iterate_category(category_name, category_prefix, error_log_path):\n",
    "    folder_path = f'./myDataset/{category_name}'\n",
    "    files = sorted(os.listdir(folder_path))[:100]  \n",
    "    all_data = {}\n",
    "    errors = []\n",
    "\n",
    "    for idx, file_name in enumerate(files, start=1):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                svos = return_svo(text)\n",
    "                if svos:\n",
    "                    all_data[f\"{category_prefix}-{idx}\"] = {\n",
    "                        \"SVO_relationships\": svos,\n",
    "                        \"total_SVOs\": len(svos)\n",
    "                    }\n",
    "                else:\n",
    "                    errors.append(f\"No SVOs found in file {file_name} ({category_prefix}-{idx})\")\n",
    "        except Exception as e:\n",
    "            errors.append(f\"Error processing file {file_name} ({category_prefix}-{idx}): {str(e)}\")\n",
    "\n",
    "    output_json_path = f'./myResult/Using_NLTK/{category_name}.json'\n",
    "    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(all_data, json_file, indent=4, ensure_ascii=False)\n",
    "    print(f\"SVOs for {category_name} saved to {output_json_path}\")\n",
    "\n",
    "    # Write errors to log file\n",
    "    with open(error_log_path, 'a', encoding='utf-8') as log_file:\n",
    "        for error in errors:\n",
    "            log_file.write(error + '\\n')\n",
    "    print(f\"Errors for {category_name} logged to {error_log_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "027af8c7-85de-4bee-b873-e61491f12b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVOs for financial saved to ./myResult/Using_NLTK/financial.json\n",
      "Errors for financial logged to ./myResult/Using_NLTK/error_log.txt\n",
      "SVOs for literature saved to ./myResult/Using_NLTK/literature.json\n",
      "Errors for literature logged to ./myResult/Using_NLTK/error_log.txt\n",
      "SVOs for medical saved to ./myResult/Using_NLTK/medical.json\n",
      "Errors for medical logged to ./myResult/Using_NLTK/error_log.txt\n",
      "SVOs for movies saved to ./myResult/Using_NLTK/movies.json\n",
      "Errors for movies logged to ./myResult/Using_NLTK/error_log.txt\n",
      "SVOs for news saved to ./myResult/Using_NLTK/news.json\n",
      "Errors for news logged to ./myResult/Using_NLTK/error_log.txt\n"
     ]
    }
   ],
   "source": [
    "# Error log path\n",
    "error_log_path = './myResult/Using_NLTK/error_log.txt'\n",
    "os.makedirs(os.path.dirname(error_log_path), exist_ok=True)\n",
    "categories = {\n",
    "    \"financial\": \"Fin\",\n",
    "    \"literature\": \"Lit\",\n",
    "    \"medical\": \"Med\",\n",
    "    \"movies\": \"Mov\",\n",
    "    \"news\": \"New\",\n",
    "}\n",
    "\n",
    "# Process each category\n",
    "for category, prefix in categories.items():\n",
    "    iterate_category(category, prefix, error_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45147be-34e6-40b7-bb98-edd974b8e8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
